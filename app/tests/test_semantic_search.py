import pandas as pd
import numpy as np
import torch
from sentence_transformers import SentenceTransformer, util
import ast
import time

# C·∫•u h√¨nh
USE_NPY = True  # ƒê·ªïi th√†nh False n·∫øu d√πng vectors_clean.csv
CSV_PATH = "data_test/vectors_clean.csv"
NPY_PATH = "data_test/vectors.npy"
TOP_K = 5
MODEL_NAME = "all-MiniLM-L6-v2"

def load_from_csv(csv_path):
    print("ƒêang load vector t·ª´ CSV...")
    df = pd.read_csv(csv_path)
    vectors = df["vector"].apply(ast.literal_eval).tolist()
    ids = df["id"].tolist()
    embeddings = torch.tensor(vectors, dtype=torch.float32)
    return embeddings, ids

def load_from_npy(npy_path, csv_path):
    print("ƒêang load vector t·ª´ NPY...")
    embeddings = torch.from_numpy(np.load(npy_path))
    df = pd.read_csv(csv_path)
    ids = df["id"].tolist()
    return embeddings, ids

def semantic_search(query_texts):
    print(f"ƒêang load model: {MODEL_NAME}...")
    model = SentenceTransformer(MODEL_NAME)

    print("ƒêang encode query...")
    query_embeddings = model.encode(query_texts, convert_to_tensor=True)
    query_embeddings = util.normalize_embeddings(query_embeddings)

    if USE_NPY:
        corpus_embeddings, corpus_ids = load_from_npy(NPY_PATH, CSV_PATH)
    else:
        corpus_embeddings, corpus_ids = load_from_csv(CSV_PATH)

    corpus_embeddings = util.normalize_embeddings(corpus_embeddings)

    print("üîç ƒêang th·ª±c hi·ªán semantic search...")
    start = time.time()
    hits = util.semantic_search(query_embeddings, corpus_embeddings, top_k=TOP_K)
    duration = time.time() - start
    print(f"‚úÖ T√¨m ki·∫øm xong trong {duration:.2f} gi√¢y")

    data_file = "data_test/WebScrapData_rows.csv"

    # Hi·ªÉn th·ªã k·∫øt qu·∫£ k√®m th√¥ng tin m√¥ t·∫£
    for idx, hit_list in enumerate(hits):
        print(f"\nüîé Query: {query_texts[idx]}")
        hit_ids = [corpus_ids[hit['corpus_id']] for hit in hit_list]

        # Truy xu·∫•t th√¥ng tin t∆∞∆°ng ·ª©ng
        info_list = query_csv_by_ids(data_file, hit_ids)
        info_dict = {item["id"]: item for item in info_list}

        for rank, hit in enumerate(hit_list, 1):
            doc_id = corpus_ids[hit['corpus_id']]
            score = hit["score"]
            info = info_dict.get(doc_id, {})

            headline = info.get("headline", "N/A")
            desc = info.get("short_description", "N/A")
            print(f"{rank}. ID: {doc_id} | Score: {score:.4f}")
            print(f"   üì∞ Headline: {headline}")
            print(f"   üìå Description: {desc}\n")


def query_csv_by_ids(csv_file, ids):
    try:
        # ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV
        df = pd.read_csv(csv_file)

        # L·ªçc c√°c d√≤ng c√≥ ID n·∫±m trong danh s√°ch ids
        filtered_df = df[df['id'].isin(ids)]

        # L·∫•y c√°c tr∆∞·ªùng c·∫ßn thi·∫øt: id, headline, short_description
        results = filtered_df[['id', 'headline', 'short_description']].to_dict(orient='records')

        return results

    except Exception as e:
        print(f"Error occurred while reading CSV: {str(e)}")
        return None

if __name__ == "__main__":
    csv_file = "data_test/WebScrapData_rows.csv"
    # V√≠ d·ª• query
    queries = ["Messi leave Barcelona"]
    semantic_search(queries)
